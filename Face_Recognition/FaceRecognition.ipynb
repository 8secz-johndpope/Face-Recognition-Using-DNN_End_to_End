{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceRecognition.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBewzE1nk1gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQn1GWIlk22C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn, optim, as_tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.init import *\n",
        "from torchvision import transforms, utils, datasets, models\n",
        "#from models.inception_resnet_v1 import InceptionResnetV1\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from pdb import set_trace\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage import io, transform\n",
        "from tqdm import trange, tqdm\n",
        "import csv\n",
        "import glob\n",
        "import dlib\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dzw4Xe7nQpY",
        "colab_type": "text"
      },
      "source": [
        "##Copy Dataset to Memory and unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbkktr9AnU3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp '/content/gdrive/My Drive/Face-Recognition-Using-DNN_End_to_End/Face_Recognition/dataset/lfw_funneled_ganji.zip' lfw_funneled_ganji.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDA685DCngTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip lfw_funneled_ganji.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzAcQijjmU24",
        "colab_type": "text"
      },
      "source": [
        "### Train the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYpxGhpsmUOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/lfw_funneled_ganji'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
        "                                              batch_size=8, \n",
        "                                             shuffle=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "class_names\n",
        "len(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leACTZVeoKak",
        "colab_type": "text"
      },
      "source": [
        "##Show the images in a grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPd9onvEoNOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "# Make a grid from batch\n",
        "out = utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o07Y2wTboVrE",
        "colab_type": "text"
      },
      "source": [
        "##Get pretrained resnet on VggFace2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-A91fTPqO1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Face-Recognition-Using-DNN_End_to_End/Face_Recognition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnwNzw75ob7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#import faceapp_models as facemodels\n",
        "#from faceapp_models import inception_resnet_v1\n",
        "from faceapp_models.inception_resnet_v1 import InceptionResnetV1\n",
        "#import inception_resnet_v1.InceptionResnetV1\n",
        "#from models.inception_resnet_v1 import InceptionResnetV1\n",
        "print('Running on device: {}'.format(device))\n",
        "model_ft = InceptionResnetV1(pretrained='vggface2', classify=False, num_classes = len(class_names))\n",
        "#model_ft = InceptionResnetV1(classify=True, num_classes = len(class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inf1ShgpZ-Cv",
        "colab_type": "text"
      },
      "source": [
        "##Freeze early layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7LoC3l7Z__Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(model_ft.children())[-6:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrcmNR7-aEXd",
        "colab_type": "text"
      },
      "source": [
        "##Remove the last layers after conv block and place in layer_list ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiD2XeYaaC2y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-HKE2ofaJF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_list = list(model_ft.children())[-5:] # all final layers\n",
        "layer_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMGZFnitaPn7",
        "colab_type": "text"
      },
      "source": [
        "##Put all beginning layers in an nn.Sequential . model_ft is now a torch model but without the final linear, pooling, batchnorm, and sigmoid layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYZA0fa2aQxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = nn.Sequential(*list(model_ft.children())[:-5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cr4-IGjad_7",
        "colab_type": "text"
      },
      "source": [
        "##If training just final layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHd2pk0nafMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model_ft.parameters():\n",
        "    #print('before-',param)\n",
        "    param.requires_grad = False\n",
        "    #print('after-',param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE7R_jH2ao0A",
        "colab_type": "text"
      },
      "source": [
        "##Re-attach the last 5 layers which automatically sets requires_grad = True .\n",
        "This linear layer Linear(in_features=1792, out_features=512, bias=False) actually requires writing two custom classes which is not entirely obvious by looking at it, but if you look at the data input/output you can see that there is a Flatten and normalize class within the layer. Check resnet implementation for reason of reshaping in last_linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmkVTKdYaqSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "class normalize(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(normalize, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSYSWJMnayo1",
        "colab_type": "text"
      },
      "source": [
        "##Then you can apply the final layers back to the new Sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFeF6_jkazdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft.avgpool_1a = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "model_ft.last_linear = nn.Sequential(\n",
        "    Flatten(),\n",
        "    nn.Linear(in_features=1792, out_features=512, bias=False),\n",
        "    normalize()\n",
        ")\n",
        "#model_ft.logits = nn.Linear(layer_list[3].in_features, len(class_names))\n",
        "model_ft.logits = nn.Linear(layer_list[4].in_features, len(class_names))\n",
        "model_ft.softmax = nn.Softmax(dim=1)\n",
        "model_ft = model_ft.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-2, momentum=0.9)\n",
        "# Decay LR by a factor of *gamma* every *step_size* epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aelMQ0E4bFxw",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZQWe-bYbHi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler,\n",
        "                num_epochs=25):\n",
        "    since = time.time()\n",
        "    FT_losses = []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "    # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        scheduler.step()\n",
        "                \n",
        "                FT_losses.append(loss.item())\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, FT_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZwqPz2SbJz_",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ai4DA3-bLDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft, FT_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=500)\n",
        "#model_ft, FT_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=500)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"FRT Loss During Training\")\n",
        "plt.plot(FT_losses, label=\"FT loss\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}